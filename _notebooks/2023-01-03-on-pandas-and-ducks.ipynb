{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fd1160",
   "metadata": {},
   "source": [
    "# On pandas and ducks\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [pandas, database]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- annotations: true\n",
    "\n",
    "> We use DuckDB for high-performance aggregates on Pandas dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443e914",
   "metadata": {},
   "source": [
    "## The problem\n",
    "\n",
    "In many analytical problems, we need to perform aggregates where the aggregation key is of very high cardinality. For example, in website actvity data, we might be interested in the duration of activity for each user. So we might perform a `group-by` operation on user_id, which can result in a very large number of groups, assuming there are many users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1a0ee3",
   "metadata": {},
   "source": [
    "### Simulate data\n",
    "\n",
    "Below we generate 10 million data points, where each user_id has on average ten records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9a0e560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8195914</th>\n",
       "      <td>290816</td>\n",
       "      <td>2020-01-02 03:42:51.951951951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8511040</th>\n",
       "      <td>817826</td>\n",
       "      <td>2020-01-03 14:32:06.786786786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id                            ts\n",
       "8195914   290816 2020-01-02 03:42:51.951951951\n",
       "8511040   817826 2020-01-03 14:32:06.786786786"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "n_items = 10_000_000\n",
    "n_keys = n_items // 10\n",
    "\n",
    "dates = pd.date_range(\"2020-01-01 05:11\", \"2020-01-03 17:22\", periods=1000)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"user_id\": np.random.choice(n_keys, size=n_items), \n",
    "    \"ts\":      np.random.choice(dates, size=n_items),\n",
    "})\n",
    "\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c1e8dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999951"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"user_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377d4e5",
   "metadata": {},
   "source": [
    "### Pandas aggregation\n",
    "\n",
    "Let's try to find the interval (in seconds) between first and last timestamp for each user using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b16417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval_seconds(s):\n",
    "    return (s.max() - s.min()).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6648c738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 58s, sys: 5.7 s, total: 3min 3s\n",
      "Wall time: 2min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "216660.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df.groupby(\"user_id\")[\"ts\"].agg(interval_seconds).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf8a23",
   "metadata": {},
   "source": [
    "This computation takes just under 3 minutes on my machine. When the number of group keys is very large, pandas does not perform well. Let's see if we can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b90bf",
   "metadata": {},
   "source": [
    "## DuckDB for the aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6065ac85",
   "metadata": {},
   "source": [
    "[DuckDB](https://duckdb.org/) is an in-process column-based database. It is dubbed \"SQlLite for analytics\" because it is embedded into the application - there is no separate database server to be managed. It promises great performance for analytical queries on large datasets.\n",
    "\n",
    "More specifically for our use-case, there is a strong integration between DuckDB and Pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a47542d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# connect to an in-memory database\n",
    "con = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9595457a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.12 s, sys: 799 ms, total: 6.92 s\n",
      "Wall time: 909 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "216660"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"\"\"\n",
    "with deltas AS (\n",
    "    select \n",
    "      user_id, \n",
    "      extract('epoch' from max(ts) - min(ts)) AS delta_seconds\n",
    "    from df\n",
    "    group by user_id\n",
    ")\n",
    "    select max(delta_seconds) AS m\n",
    "    FROM deltas\n",
    "\"\"\"\n",
    "results = con.execute(query).fetchone()\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da325440",
   "metadata": {},
   "source": [
    "We perform a query directly on a pandas dataframe, without inserting it into the database first. This works because DuckDB uses Apache Arrow to transfer data efficiently with pandas.\n",
    "\n",
    "The DuckDB query completes in less than a second, or approximately 180 times faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb96af",
   "metadata": {},
   "source": [
    "### Get the aggregation result back as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0fea8",
   "metadata": {},
   "source": [
    "In the example above, we return a single number, the max duration. But what if we wanted to get all durations back as a pandas dataframe? Maybe we want to plot the results, or continue the analysis. Fortunately, this is also easy to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8fe14e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.46 s, sys: 8.26 ms, total: 5.47 s\n",
      "Wall time: 776 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>delta_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>652093</td>\n",
       "      <td>174369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>308985</td>\n",
       "      <td>200394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  delta_seconds\n",
       "0   652093         174369\n",
       "1   308985         200394"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"\"\"\n",
    "    select \n",
    "      user_id, \n",
    "      extract('epoch' from max(ts) - min(ts)) AS delta_seconds\n",
    "    from df\n",
    "    group by user_id\n",
    "\"\"\"\n",
    "result_df = con.execute(query).df()\n",
    "result_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d4f300",
   "metadata": {},
   "source": [
    "## Does the speed-up really matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad87323",
   "metadata": {},
   "source": [
    "We saw some impressive speed-up when using DuckDB for the aggregation. But we must ask ourselves if this speed-up really impacts our work. \n",
    "\n",
    "I like to divide computational tasks into the following categories.\n",
    "\n",
    "* Interactive work: < 1 minute\n",
    "* Slower tasks: < 1 hour\n",
    "* Batch tasks: more than 1 hour, could be days.\n",
    "\n",
    "### Interactive workloads\n",
    "Interactive workloads finish within a minute or so. Typically, people are most productive in this category because they can pose a question (in the form of a query or computational task) and get answers very quickly. They can follow their train of thoughts without major interruptions or distractions.\n",
    "\n",
    "### Slower tasks\n",
    "\n",
    "### Batch tasks\n",
    "Batch tasks require a significant amount of time and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada53db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
